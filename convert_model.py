import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from transformers import WhisperProcessor
import openvino as ov # å¯¼å…¥ openvino åº“

safe_temp_dir = "C:/AI_zimu_jihua/code/ChickenRice_v2/models/temp"
if not os.path.exists(safe_temp_dir):
    os.makedirs(safe_temp_dir)
# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TEMP'] = safe_temp_dir
os.environ['TMP'] = safe_temp_dir
print(f"!!! Forcing temporary directory to: {safe_temp_dir} to avoid encoding errors.")

# --- é…ç½® ---
# 1. ä½ çš„ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
PYTORCH_MODEL_PATH = "C:/AI_zimu_jihua/code/ChickenRice_v2/models/whisper-large-v2-translate-zh-v0.2-st"

# 2. ä¸­é—´ ONNX æ¨¡å‹çš„ä¿å­˜è·¯å¾„
ONNX_MODEL_PATH = "C:/AI_zimu_jihua/code/ChickenRice_v2/models/temp_onnx_model"

# 3. æœ€ç»ˆ OpenVINO æ¨¡å‹çš„ä¿å­˜è·¯å¾„
OV_MODEL_PATH = "C:/AI_zimu_jihua/code/ChickenRice_v2/models/whisper-chickenrice-large-v2-ov"

# --- æ­¥éª¤ 1: å¯¼å‡ºåˆ° ONNX ---
print("="*60)
print(f"æ­¥éª¤ 1: å¼€å§‹å°† PyTorch æ¨¡å‹å¯¼å‡ºåˆ° ONNX...")
print(f"  - è¾“å…¥æ¨¡å‹: {PYTORCH_MODEL_PATH}")
print(f"  - è¾“å‡º ONNX è·¯å¾„: {ONNX_MODEL_PATH}")

try:
    # ä½¿ç”¨ ORTModelForSpeechSeq2Seq æ¥è¿›è¡Œ ONNX çš„è½¬æ¢
    ort_model = ORTModelForSpeechSeq2Seq.from_pretrained(PYTORCH_MODEL_PATH, export=True)
    # ä¿å­˜ ONNX æ¨¡å‹æ–‡ä»¶
    ort_model.save_pretrained(ONNX_MODEL_PATH)
    
    # åŒæ—¶ä¿å­˜å¤„ç†å™¨é…ç½®
    processor = WhisperProcessor.from_pretrained(PYTORCH_MODEL_PATH)
    processor.save_pretrained(ONNX_MODEL_PATH) # å…ˆå’ŒONNXæ”¾ä¸€èµ·

    print("æ­¥éª¤ 1: ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼")
    print("="*60)

except Exception as e:
    print(f"æ­¥éª¤ 1: ONNX æ¨¡å‹å¯¼å‡ºå¤±è´¥ï¼é”™è¯¯: {e}")
    # å¦‚æœç¬¬ä¸€æ­¥å°±å¤±è´¥ï¼Œå°±ä¸ç»§ç»­äº†
    exit(1)


# --- æ­¥éª¤ 2: å°† ONNX è½¬æ¢ä¸º OpenVINO IR ---
print("\n" + "="*60)
print(f"æ­¥éª¤ 2: å¼€å§‹å°† ONNX æ¨¡å‹è½¬æ¢ä¸º OpenVINO IR...")
print(f"  - è¾“å…¥ ONNX æ¨¡å‹: {Path(ONNX_MODEL_PATH) / 'encoder_model.onnx'}")
print(f"  - è¾“å‡º OpenVINO è·¯å¾„: {OV_MODEL_PATH}")

# ç¡®ä¿æœ€ç»ˆè¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OV_MODEL_PATH, exist_ok=True)

try:
    # Whisper ONNX æ¨¡å‹ç”±3éƒ¨åˆ†ç»„æˆ: encoder, decoder, decoder_with_past
    # æˆ‘ä»¬éœ€è¦åˆ†åˆ«è½¬æ¢å®ƒä»¬
    onnx_models_to_convert = {
        "encoder_model.onnx": "openvino_encoder_model.xml",
        "decoder_model.onnx": "openvino_decoder_model.xml",
        "decoder_with_past_model.onnx": "openvino_decoder_with_past_model.xml"
    }

    core = ov.Core()

    for onnx_name, ov_name in onnx_models_to_convert.items():
        onnx_file = Path(ONNX_MODEL_PATH) / onnx_name
        ov_file = Path(OV_MODEL_PATH) / Path(ov_name).with_suffix(".xml")
        
        if not onnx_file.exists():
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ° {onnx_file}ï¼Œè·³è¿‡è½¬æ¢ã€‚")
            continue
            
        print(f"  - æ­£åœ¨è½¬æ¢: {onnx_name} -> {ov_name}")
        
        # åŠ è½½ ONNX æ¨¡å‹
        model = core.read_model(model=str(onnx_file))
        
        # è½¬æ¢å¹¶ä¿å­˜ä¸º OpenVINO IR æ ¼å¼ (.xml å’Œ .bin)
        ov.save_model(model, output_model=str(ov_file))

    # å°†é…ç½®æ–‡ä»¶ä¹Ÿå¤åˆ¶åˆ°æœ€ç»ˆç›®å½•
    print("  - æ­£åœ¨å¤åˆ¶æ¨¡å‹é…ç½®æ–‡ä»¶...")
    import shutil
    for filename in os.listdir(ONNX_MODEL_PATH):
        if filename.endswith(".json"):
            shutil.copy(Path(ONNX_MODEL_PATH) / filename, Path(OV_MODEL_PATH) / filename)
            
    # ç‰¹åˆ«åœ°ï¼ŒæŠŠ processor çš„æ–‡ä»¶ä¹Ÿå¤åˆ¶è¿‡å»
    processor.save_pretrained(OV_MODEL_PATH)

    print("æ­¥éª¤ 2: OpenVINO IR è½¬æ¢æˆåŠŸï¼")
    print("="*60)

    print(f"\nğŸ‰ è½¬æ¢å…¨éƒ¨å®Œæˆï¼ä½ çš„ OpenVINO æ¨¡å‹å·²ä¿å­˜åœ¨: {OV_MODEL_PATH}")

except Exception as e:
    print(f"æ­¥éª¤ 2: ONNX åˆ° OpenVINO çš„è½¬æ¢å¤±è´¥ï¼é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

